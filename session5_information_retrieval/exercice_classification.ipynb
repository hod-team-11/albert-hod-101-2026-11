{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dccfc428",
   "metadata": {},
   "source": [
    "## Classification ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527c7800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RAPPORT DE CLASSIFICATION ---\n",
      "                      Précision  Rappel  F1-Score  Support\n",
      "animalerie                0.863   0.843     0.853    134.0\n",
      "auto et moto              0.724   0.656     0.689     64.0\n",
      "bebe et puericulture      0.458   0.846     0.595     13.0\n",
      "cuisine et maison         0.917   0.859     0.887    205.0\n",
      "epicerie                  0.903   0.814     0.856    172.0\n",
      "high-tech                 0.931   0.950     0.940    199.0\n",
      "hygiene et sante          0.500   0.640     0.561     25.0\n",
      "jeux video                0.000   0.000     0.000     13.0\n",
      "vetements                 0.878   0.903     0.890    175.0\n",
      "\n",
      "Accuracy Globale : 84.50%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "path_truth = os.path.join(current_dir, 'fichiers_csv', 'classification dataset - ground_truth.csv')\n",
    "\n",
    "df = pd.read_csv(path_truth)\n",
    "\n",
    "# on nettoie les données en supprimant les lignes avec des valeurs manquantes\n",
    "y_true = df['real_category'].astype(str).tolist()\n",
    "y_pred = df['prediction'].astype(str).tolist()\n",
    "categories = sorted(list(set(y_true)))\n",
    "\n",
    "# initialisation du dictionnaire pour stocker les métriques\n",
    "metrics = {}\n",
    "#inistialisation des compteurs\n",
    "for cat in categories:\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    # calculs des différents compteurs\n",
    "    for i in range(len(y_true)):\n",
    "        # si la prédiction est correcte\n",
    "        if y_pred[i] == cat and y_true[i] == cat:\n",
    "            tp += 1\n",
    "        # si la prédiction est différente de la vraie catégorie\n",
    "        elif y_pred[i] == cat and y_true[i] != cat:\n",
    "            fp += 1\n",
    "        # si la vraie catégorie est différente de la prédiction\n",
    "        elif y_pred[i] != cat and y_true[i] == cat:\n",
    "            fn += 1\n",
    "            \n",
    "    # Calcul Précision, Recall et F1-score\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    metrics[cat] = {\n",
    "        'Précision': round(precision, 3),\n",
    "        'Recall': round(recall, 3),\n",
    "        'F1-Score': round(f1, 3),\n",
    "        'Support': (y_true.count(cat))\n",
    "    }\n",
    "\n",
    "# 3. Affichage des résultats\n",
    "report = pd.DataFrame(metrics).T\n",
    "print(\"--- RAPPORT DE CLASSIFICATION ---\")\n",
    "print(report)\n",
    "\n",
    "# on calcule l'accuracy globale\n",
    "accuracy = sum(1 for i in range(len(y_true)) if y_true[i] == y_pred[i]) / len(y_true)\n",
    "print(f\"\\nAccuracy Globale : {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
