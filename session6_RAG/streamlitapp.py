import streamlit as st
import os
import uuid
from langchain_community.vectorstores import Chroma
from langchain_mistralai import MistralAIEmbeddings, ChatMistralAI
from langchain_text_splitters import RecursiveCharacterTextSplitter 
from langchain_core.documents import Document
from langchain_classic.chains import ConversationalRetrievalChain
from langchain_classic.memory import ConversationBufferMemory

# configuration initiale de la page streamlit
st.set_page_config(page_title="Othello AI - Mistral", layout="wide")

# le session state sert √† garder l'historique des discussions
if "chats" not in st.session_state:
    #ici on initialise un dictionnaire vide pour stocker les discussions
    st.session_state.chats = {}
# si aucune discussion n'est s√©lectionn√©e on initialise la variable √† None, de fait on cr√©e une nouvelle discussion
if "current_chat_id" not in st.session_state:
    st.session_state.current_chat_id = None

# on d√©finit la fonction pour le RAG
def load_text():
    # sur le site on a le texte de othello dans un fichier txte utf-8. On doit donc sp√©cifier l'encodage
    with open("othello.txt", "r", encoding="utf-8") as f:
        text = f.read()
    # on d√©coupe le texte en parties pour le RAG afin d'am√©liorer la pertinence des r√©ponses
    #la taille du chunk est de 500 caract√®res comme sp√©cifi√© dans l'assignment
    # l'overlap est de 100 caract√®res pour garder du contexte entre les chunks. 
    # (il r√©cup√®re les 100 derniers caract√®res du chunk pr√©c√©dent afin d'aml√©liorer la coh√©rence)
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    # on cr√©e une liste de documents √† partir des chunks
    return [Document(page_content=chunk) for chunk in splitter.split_text(text)]

# le @st.cache_resourceest un d√©corateur qui permet de ne pas recalculer la base de donn√©es √† chaque interaction
@st.cache_resource
# on d√©finit la fonction qui configure la base de donn√©es vectorielle
def setup_db(api_key):
    # on utilise les embeddings Mistral AI (chaque IA √† son propre mod√®le d'embedding mistral n'a pas le meme que chatgpt)
    embeddings = MistralAIEmbeddings(mistral_api_key=api_key)
    # on d√©finit le directory o√π sera stock√©e la base de donn√©es chromadb
    persist_dir = "./chroma_db_mistral"
    # on test se la base de donn√©es existe d√©j√†
    if os.path.exists(persist_dir):
        # si oui on la charge
        return Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    else:
        # sinon on la cr√©e √† partir des documents
        docs = load_text()
        return Chroma.from_documents(docs, embeddings, persist_directory=persist_dir)

#on d√©finit la fonction pour cr√©er une nouvelle discussion
def create_new_chat():
    # chaque discussion a un id unique
    new_id = str(uuid.uuid4())
    # le titre par d√©faut est Discussion + num√©ro de la discussion
    # chaque discussion a son propre historique de messages et sa propre m√©moire
    st.session_state.chats[new_id] = {
        "title": f"Discussion {len(st.session_state.chats) + 1}",
        "messages": [],
        "memory": ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key='answer')
    }
    # on met √† jour l'id de la discussion courante
    st.session_state.current_chat_id = new_id

# on cr√©e la barre lat√©rale
with st.sidebar:
    # titre de l'application
    st.title("üìö Othello AI")
    
    # cr√©ation du bouton pour la nouvelle disccussion avec du markdown pour le visuel
    if st.button("‚ûï Nouvelle discussion", use_container_width=True):
        # on appelle la fonction pour cr√©er une nouvelle discussion
        create_new_chat()
    
    # st.divider cr√©e une ligne de s√©paration dans la barre lat√©rale afin de mieux organiser les disucssions
    st.divider()
    # le titre de la section est historique pour les discussions pr√©c√©dentes
    st.subheader("Historique")
    # on fait une boucle sur les discussions existantes
    for chat_id, chat_data in st.session_state.chats.items():
        # Bouton pour switcher entre les discussions
        if st.button(f"üí¨ {chat_data['title']}", key=chat_id, use_container_width=True):
            # on met √† jour l'id de la discussion courante pour afficher l'historique qui correspond
            st.session_state.current_chat_id = chat_id
    
    # on remet une s√©paration pour une nouvelle section
    st.divider()
    # initialisation de la navigation entre les pages
    # on a une page d'accueil, une page pour le chatbot et une page pour le comparatif des mod√®les
    page = st.radio("Navigation", ["üè† Accueil", "üí¨ Chatbot", "üß† Comparatif Mod√®les"])
    # c'est l'espace pour que l'utilisateur entre sa cl√© Mistral AI
    api_key = st.text_input("üîë Cl√© API Mistral", type="password")

# si la page s√©lectionn√©e est l'accueil
# on affiche le titre et les instructions relatives √† l'utilisation de l'application
if page == "üè† Accueil":
    st.title("üé≠ Analyse d'Othello avec Mistral AI")
    st.write("Pour utiliser ce chatbot, entrez votre cl√© API Mistral AI dans la barre lat√©rale. " \
    "Cr√©ez une nouvelle discussion et posez vos questions sur le texte d'Othello. " \
    "Le chatbot utilise la technique de RAG (Retrieval-Augmented Generation) pour fournir des r√©ponses pr√©cises bas√©es sur le texte." \
    "\n\n" "si vous n'avez pas de cl√© API, vous pouvez en obtenir une gratuitement sur le site de Mistral AI.")
    st.markdown("lien : [Mistral AI](https://console.mistral.ai/home)")
# si la page s√©lectionn√©e est le comparatif des mod√®les alors on affiche un tableau comparatif entre les deux mod√®les mistral disponibles
elif page == "üß† Comparatif Mod√®les":
    st.title("Comparatif des mod√®les Mistral")
    st.write("Voici les deux mod√®les disponibles dans ce chatbot :")
    st.table({
        "Mod√®le": ["mistral-small-latest", "mistral-large-latest"],
        "Force": ["Vitesse & Efficacit√©", "Raisonnement complexe & Nuances"],
        "Usage": ["Questions simples", "Analyses litt√©raires pouss√©es"]
    })
# si la page s√©lectionn√©e est le chatbot
elif page == "üí¨ Chatbot":
    # on v√©rifie que l'utilisateur a bien entr√© sa cl√© API
    if not api_key:
        st.error("Veuillez entrer votre cl√© API Mistral.")
    # on v√©rifie qu'une discussion est s√©lectionn√©e
    elif st.session_state.current_chat_id is None:
        st.info("Cliquez sur '‚ûï Nouvelle discussion' pour d√©marrer.")
    # si une discussion est s√©lectionn√©e on affiche l'historique des messages
    else:
        chat = st.session_state.chats[st.session_state.current_chat_id]
        # on affiche le titre de la discussion
        st.title(f"Discussion : {chat['title']}")
        # on laisse le choix du mod√®le √† l'utilisateur
        model_choice = st.selectbox("Mod√®le Mistral", ["mistral-small-latest", "mistral-large-latest"])
        # on configure la base de donn√©es vectorielle
        v_db = setup_db(api_key)

        # affichage de l'historique des messages de la discussion selectionn√©e
        for m in chat["messages"]:
            # affichage des messages pr√©c√©dents. m["role"] sert √† diff√©rencier les messages utilisateur et agent
            with st.chat_message(m["role"]):
                # on affiche le contenu du chat en markdown
                st.markdown(m["content"])
        #le prompt pour que l'utilisateur entre sa question
        if prompt := st.chat_input("Posez votre question sur Othello"):
            # ajout du message utilisateur √† l'historique
            chat["messages"].append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            # on d√©finit la r√©ponse de l'agent
            with st.chat_message("assistant"):
                # le spinner c'est le message qui s'affiche pendant le traitement
                with st.spinner("Analyse du texte..."):
                    # initialisation de la chaine question-r√©ponse
                    qa_chain = ConversationalRetrievalChain.from_llm(
                        llm=ChatMistralAI(mistral_api_key=api_key, model=model_choice),
                        retriever=v_db.as_retriever(search_kwargs={"k": 3}), # On prend les 3 meilleures sources
                        memory=chat["memory"],
                        return_source_documents=True
                    )
                    # on utilise invoke pour appeler la chaine avec la question de l'utilisateur
                    response = qa_chain.invoke({"question": prompt})
                    # on r√©cup√®re la r√©ponse g√©n√©r√©e par l'agent
                    answer = response["answer"]
                    # on r√©cup√®re les documents sources utilis√©s pour g√©n√©rer la r√©ponse
                    source_docs = response["source_documents"]

                    # on formate les sources pour avoir seulement celles qui nous ont servi √† r√©pondre
                    source_text = "\n\n**üîç Sources utilis√©es pour cette r√©ponse :**\n"
                    # on affiche les 3 premiers extraits de chaque document source
                    for i, doc in enumerate(source_docs):
                        # on d√©finit une limite de 3 extraits pour pas surchager la r√©ponse
                        source_text += f"**Extrait {i+1} :** {doc.page_content[:200]}...\n"
                    
                    # la r√©ponse complete c'est la r√©ponse + les sources
                    full_response = answer + source_text
                    st.markdown(full_response)
                    
                    # on sauve l'historique en ajoutant la r√©ponse de l'agent dans la liste chat["messages"]
                    chat["messages"].append({"role": "assistant", "content": full_response})
                    
                    # pour renommer la discussion on fait comme toutes les IA, on prend les 30 premiers caract√®res du premier prompt
                    if len(chat["messages"]) == 2:
                        chat["title"] = prompt[:30] + "..."
                        st.rerun()