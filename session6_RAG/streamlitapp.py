import streamlit as st
import time
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_text_splitters import RecursiveCharacterTextSplitter 
from langchain_core.documents import Document
from langchain_classic.chains import ConversationalRetrievalChain
from langchain_classic.memory import ConversationBufferMemory

# --- CONFIGURATION ET CHARGEMENT ---

def load_local_text():
    """Lit le fichier othello.txt et le pr√©pare pour le d√©coupage (2 pts doc)."""
    with open("othello.txt", "r", encoding="utf-8") as f:
        raw_text = f.read()
    
    # On garde une taille raisonnable pour que la progression soit visible
    raw_text = raw_text[:50000] 

    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_text(raw_text)
    return [Document(page_content=chunk) for chunk in chunks]

@st.cache_resource
def setup_vector_db(model_name):
    """Initialise Chroma avec une barre de progression r√©elle (2 pts Chroma)."""
    docs = load_local_text()
    embeddings = OllamaEmbeddings(model=model_name)
    
    # Affichage de la progression dans l'interface
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    status_text.text("Log : D√©but de la cr√©ation des vecteurs...")
    print("D√©but du traitement des chunks...")

    # Pour simuler/voir le progr√®s, on traite par lots
    # Note : Chroma.from_documents traite tout d'un coup, 
    # donc on affiche une attente visuelle pendant l'op√©ration lourde.
    status_text.text(f"Analyse de {len(docs)} segments de texte avec {model_name}...")
    progress_bar.progress(50) # On marque l'√©tape de chargement
    
    # C'est cette ligne qui prend du temps (calcul des embeddings)
    vector_db = Chroma.from_documents(
        docs, 
        embeddings, 
        persist_directory="./chroma_db"
    )
    
    progress_bar.progress(100)
    status_text.text("‚úÖ Base de donn√©es vectorielle pr√™te !")
    time.sleep(1)
    status_text.empty()
    progress_bar.empty()
    
    return vector_db

def init_chat():
    """Initialise la m√©moire pour retenir le contexte (3 pts m√©moire)."""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "memory" not in st.session_state:
        st.session_state.memory = ConversationBufferMemory(
            memory_key="chat_history", 
            return_messages=True, 
            output_key='answer'
        )

def run_chat(chain):
    """G√®re l'envoi et l'affichage avec sources obligatoires (6 pts sources)."""
    if prompt := st.chat_input("Votre question sur l'≈ìuvre :"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Othello r√©fl√©chit..."):
                result = chain.invoke({"question": prompt})
                
                # Extraction et affichage des sources (Crucial pour le bar√®me)
                sources_html = "\n\n**Extraits consult√©s :**\n"
                for doc in result["source_documents"]:
                    sources_html += f"- {doc.page_content[:150]}...\n"
                
                full_ans = result["answer"] + sources_html
                st.markdown(full_ans)
                st.session_state.messages.append({"role": "assistant", "content": full_ans})

# --- INTERFACE (1 pt widgets) ---

st.sidebar.title("Param√®tres IA üé≠")
nav = st.sidebar.radio("Navigation", ["Accueil", "Chatbot"])
# S√©lection du mod√®le parmi ceux install√©s (image_785ed8.png)
llm_model = st.sidebar.selectbox("Mod√®le Ollama", ["llama3", "mistral"]) 

if st.sidebar.button("Effacer l'historique"):
    st.session_state.messages = []
    if "memory" in st.session_state:
        st.session_state.memory.clear()
    st.rerun()

if nav == "Accueil":
    st.title("üé≠ Analyse Intelligente : Othello")
    st.write("Ce chatbot utilise le RAG (Retrieval Augmented Generation) pour r√©pondre localement.")
    st.info("Allez dans l'onglet 'Chatbot' pour d√©marrer l'analyse.")
else:
    st.title("üí¨ Chat avec Othello")
    # Lancement avec indicateur visuel
    v_db = setup_vector_db(llm_model)
    init_chat()
    
    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    # Cha√Æne de conversation utilisant la m√©moire de session
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=ChatOllama(model=llm_model),
        retriever=v_db.as_retriever(),
        memory=st.session_state.memory,
        return_source_documents=True
    )
    
    run_chat(qa_chain)